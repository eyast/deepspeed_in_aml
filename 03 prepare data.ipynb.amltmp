{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare a dataset for use in a training task\n",
        "\n",
        "**Goals**: show the steps necessary to download, preprocess, and register a dataset for use with AzureML. \n",
        "\n",
        "Working with AzureML registered datasets allows you to avoid bundling the dataset with the context you send to each training node (slow) or setting up a persistent mount on your DeepSpeed Docker images (more complex and brittle). Additionally, using registered datasets offers the opportunity to log what version of a dataset a model was trained with. This means you can monitor the effects of retraining over time as your data increases or changes. \n",
        "\n",
        "A note on terminology: I'll endeavor use the term dataset to refer to the AzureML dataset object stored on AzureML and data when referencing the same information stored locally.\n",
        "\n",
        "Here we'll be working with the [CoLA](https://nyu-mll.github.io/CoLA/) dataset, on which we'll later be fine-tuning a model. We'll perform the following steps:\n",
        "\n",
        "- Download the data appropriate for the training task\n",
        "- Preprocess (tokenize) it\n",
        "- Save it locally\n",
        "- Upload the preprocessed data to the AzureML datastore where our AzureML datasets are located\n",
        "- Register the resulting files as a new AzureML dataset\n",
        "\n",
        "First, imports"
      ],
      "metadata": {},
      "id": "53414860"
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "import azureml.core\n",
        "import datasets\n",
        "import transformers"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1701733919285
        }
      },
      "id": "6d880194"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using an external config.yml\n",
        "\n",
        "Now we'll load a `src/config.yml` file that tells us the task name and data locations, this is also used by the training functions and so keeps everything in one place. More information about it is available in the companion `Train model` notebook."
      ],
      "metadata": {},
      "id": "44b5dbc3"
    },
    {
      "cell_type": "code",
      "source": [
        "with open('src/config.yml', 'r') as f:\n",
        "    config = yaml.safe_load(f)"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1701733926481
        }
      },
      "id": "3d02e7f3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load or mount data \n",
        "\n",
        "The data itself will be downloaded by the `datasets` libraries from [Huggingface](https://huggingface.co/). Loading this public dataset is quite simple but this is where you'd locally mount or otherwise access a proprietary dataset. "
      ],
      "metadata": {},
      "id": "fe3f7661"
    },
    {
      "cell_type": "code",
      "source": [
        "data = datasets.load_dataset(\"glue\", config['task'])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Downloading builder script: 100%|██████████| 28.8k/28.8k [00:00<00:00, 48.5MB/s]\nDownloading metadata: 100%|██████████| 28.7k/28.7k [00:00<00:00, 52.0MB/s]\nDownloading readme: 100%|██████████| 27.9k/27.9k [00:00<00:00, 45.5MB/s]\nDownloading data: 100%|██████████| 377k/377k [00:00<00:00, 37.3MB/s]\nGenerating train split: 100%|██████████| 8551/8551 [00:00<00:00, 18672.50 examples/s]\nGenerating validation split: 100%|██████████| 1043/1043 [00:00<00:00, 32312.97 examples/s]\nGenerating test split: 100%|██████████| 1063/1063 [00:00<00:00, 30586.16 examples/s]\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1701733944908
        }
      },
      "id": "8fe94083"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess data\n",
        "\n",
        "We will preprocess our data before we save it. In this case it means tokenizing the data to prepare it for the model specified in the `src/config.yml` file."
      ],
      "metadata": {},
      "id": "0f04dbc0-6507-4c6a-ba84-c33bc4cd55e9"
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = transformers.AutoTokenizer.from_pretrained(config['model'])\n",
        "def tokenizer_function(examples):\n",
        "    return tokenizer(examples['sentence'], padding=\"max_length\", truncation=True)\n",
        "\n",
        "data = data.map(tokenizer_function, batched=True)\n",
        "data.save_to_disk(config['data_dir'])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "tokenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 18.1kB/s]\nconfig.json: 100%|██████████| 483/483 [00:00<00:00, 269kB/s]\nvocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 14.6MB/s]\ntokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 58.1MB/s]\nMap: 100%|██████████| 8551/8551 [00:01<00:00, 6947.50 examples/s]\nMap: 100%|██████████| 1043/1043 [00:00<00:00, 9578.07 examples/s]\nMap: 100%|██████████| 1063/1063 [00:00<00:00, 9646.73 examples/s]\nSaving the dataset (1/1 shards): 100%|██████████| 8551/8551 [00:00<00:00, 50101.27 examples/s]\nSaving the dataset (1/1 shards): 100%|██████████| 1043/1043 [00:00<00:00, 13579.87 examples/s]\nSaving the dataset (1/1 shards): 100%|██████████| 1063/1063 [00:00<00:00, 11974.01 examples/s]\n"
        }
      ],
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1701733971899
        }
      },
      "id": "fabfa0e2-b545-4572-9611-dd3c2f86d689"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connect to AzureML workspace and upload data\n",
        "\n",
        "Now we'll connect to the AzureML workspace and Azure datastore where our dataset will live. We instantiate a connection to the workspace using a configuration file that is automatically provided on the AzureML compute instances but which [we must create](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-configure-environment#workspace) if we run this notebook on our own desktop or laptop machine. \n",
        "\n",
        "[Azure datastores](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.datastore.datastore?view=azure-ml-py) can be blob storage, file shares, datalakes, and more. Here we'll use the default, a file share, but any of the other backing services can be used after first [registering them with AzureML](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-access-data#create-and-register-datastores) either through the studio UI or Python SDK."
      ],
      "metadata": {},
      "id": "f862c5a4"
    },
    {
      "cell_type": "code",
      "source": [
        "workspace = azureml.core.Workspace.from_config()\n",
        "datastore = workspace.get_default_datastore()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
        }
      ],
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1701733980951
        }
      },
      "id": "28ec883b"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload data stored on local disk to the datastore. We are clobbering any prior versions in this upload, assign a UUID subdirectory or other versioning mechanism to retain prior versions of this data within the datastore."
      ],
      "metadata": {},
      "id": "cad7539b"
    },
    {
      "cell_type": "code",
      "source": [
        "datastore.upload(src_dir=config['data_dir'], \n",
        "                 target_path=config['data_dir'], \n",
        "                 overwrite=True\n",
        "                )"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "\"Datastore.upload\" is deprecated after version 1.0.69. Please use \"Dataset.File.upload_directory\" to upload your files             from a local directory and create FileDataset in single method call. See Dataset API change notice at https://aka.ms/dataset-deprecation.\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Uploading an estimated of 10 files\nUploading data/glue/cola/dataset_dict.json\nUploaded data/glue/cola/dataset_dict.json, 1 files out of an estimated total of 10\nUploading data/glue/cola/test/data-00000-of-00001.arrow\nUploaded data/glue/cola/test/data-00000-of-00001.arrow, 2 files out of an estimated total of 10\nUploading data/glue/cola/test/dataset_info.json\nUploaded data/glue/cola/test/dataset_info.json, 3 files out of an estimated total of 10\nUploading data/glue/cola/test/state.json\nUploaded data/glue/cola/test/state.json, 4 files out of an estimated total of 10\nUploading data/glue/cola/train/dataset_info.json\nUploaded data/glue/cola/train/dataset_info.json, 5 files out of an estimated total of 10\nUploading data/glue/cola/train/state.json\nUploaded data/glue/cola/train/state.json, 6 files out of an estimated total of 10\nUploading data/glue/cola/validation/data-00000-of-00001.arrow\nUploaded data/glue/cola/validation/data-00000-of-00001.arrow, 7 files out of an estimated total of 10\nUploading data/glue/cola/validation/dataset_info.json\nUploaded data/glue/cola/validation/dataset_info.json, 8 files out of an estimated total of 10\nUploading data/glue/cola/validation/state.json\nUploaded data/glue/cola/validation/state.json, 9 files out of an estimated total of 10\nUploading data/glue/cola/train/data-00000-of-00001.arrow\nUploaded data/glue/cola/train/data-00000-of-00001.arrow, 10 files out of an estimated total of 10\nUploaded 10 files\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 6,
          "data": {
            "text/plain": "$AZUREML_DATAREFERENCE_5b1633259dfc44b08d94174913652951"
          },
          "metadata": {}
        }
      ],
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1701733991893
        }
      },
      "id": "aa983db6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Register the data as a dataset\n",
        "\n",
        "Register the dataset with associated metadata describing the task for which it is meant and the model used to tokenize it."
      ],
      "metadata": {},
      "id": "a88cc1e6"
    },
    {
      "cell_type": "code",
      "source": [
        "name = config['task']\n",
        "description = f\"Glue dataset for {config['task']}, tokenized for {config['model']}\"\n",
        "tags = {\"task\":config['task'], \"model\": config['model']}\n",
        "path = datastore.path(config['data_dir'])\n",
        "\n",
        "amldataset = azureml.core.Dataset.File.from_files(path)\n",
        "amldataset = amldataset.register(workspace, name, description, tags, True) "
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
        }
      ],
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1701734276791
        }
      },
      "id": "43537570"
    },
    {
      "cell_type": "markdown",
      "source": [
        "And we'll need to know what version that created in order to use it going forward"
      ],
      "metadata": {},
      "id": "ab36c826"
    },
    {
      "cell_type": "code",
      "source": [
        "amldataset.version"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 8,
          "data": {
            "text/plain": "1"
          },
          "metadata": {}
        }
      ],
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1701734280740
        }
      },
      "id": "5f51ff26"
    },
    {
      "cell_type": "markdown",
      "source": [
        "This shows us the steps to:\n",
        "\n",
        "- Prepare data before moving it to Azure\n",
        "- Push data to an Azure datastore accessible by AzureML\n",
        "- Register the data with AzureML as a dataset"
      ],
      "metadata": {},
      "id": "6882843d"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml-pt-tf",
      "language": "python",
      "display_name": "Python 3.8 - Pytorch and Tensorflow"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python38-azureml-pt-tf"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}